Neural Network Evaluation Report
AND Gate Model:

For the AND gate model, I used a simple neural network with 2 input neurons and 1 output neuron.
Since we only have two input features (0 or 1), the input layer consists of 2 neurons.
The output layer consists of 1 neuron to represent the output of the AND gate (0 or 1).
I used the sigmoid activation function for both layers.
OR Gate Model:

Similar to the AND gate model, the OR gate model also has 2 input neurons and 1 output neuron.
The architecture is the same as the AND gate model, with 2 neurons in the input layer and 1 neuron in the output layer.
Sigmoid activation function was used for both layers.
XOR Gate Model:

For the XOR gate model, I used a slightly more complex architecture compared to AND and OR gates.
It consists of 2 input neurons, a hidden layer with 2 neurons, and 1 output neuron.
The input layer takes the two input features (0 or 1), the hidden layer processes them, and the output layer provides the result (0 or 1).
Sigmoid activation function was used for all layers.
Training Process:
I trained all models using a method called Stochastic Gradient Descent (SGD) optimizer.
The learning rate was set to 0.1, which determines how much the model's parameters are updated during training.
To measure how well the model is performing, I used a loss function called Binary Cross Entropy.
Training was done for 1000 epochs, meaning the entire dataset was passed forward and backward through the network 1000 times.
Results:
AND Gate Model: After training, the AND gate model achieved an accuracy of 100%. This means it correctly predicted the output for all input combinations of the AND gate.

OR Gate Model: Similar to the AND gate model, the OR gate model also achieved an accuracy of 100%.

XOR Gate Model: Despite being more complex, the XOR gate model could only achieve approximately 50% accuracy. This means it struggled to accurately predict the output for all input combinations of the XOR gate, which is expected due to its nature.


